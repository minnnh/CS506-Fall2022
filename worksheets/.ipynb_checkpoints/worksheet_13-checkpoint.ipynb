{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 13\n",
    "\n",
    "Name:  Min Han  \n",
    "UID: U15822408\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Support Vector Machines\n",
    "- Recommender Systems\n",
    "\n",
    "## Support Vector Machines\n",
    "\n",
    "Follow along in class to implement the perceptron algorithm and create an animation of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.5\n",
      "accuracy 0.6\n",
      "accuracy 0.6\n",
      "accuracy 0.6\n",
      "accuracy 0.6\n",
      "accuracy 0.8\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 0.8\n",
      "accuracy 1.0\n",
      "accuracy 0.8\n",
      "accuracy 0.8\n",
      "accuracy 0.8\n",
      "accuracy 0.8\n",
      "accuracy 0.8\n",
      "accuracy 0.8\n",
      "accuracy 0.8\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 0.9\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "CENTERS = [[0, 1], [1, 0]]\n",
    "\n",
    "# Dataset\n",
    "X, labels = datasets.make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\n",
    "Y = np.array(list(map(lambda x : -1 if x == 0 else 1, labels.tolist())))\n",
    "\n",
    "# Initializing w and b\n",
    "w = np.array([1, 1])\n",
    "b = 0\n",
    "\n",
    "# Perceptron Parameters\n",
    "epochs = 100\n",
    "alpha = .05\n",
    "expanding_rate = .99\n",
    "retracting_rate = 1.1\n",
    "\n",
    "def snap(x, w, b, error):\n",
    "    \"\"\"\n",
    "        Plot the street induced by w and b.\n",
    "        Circle the point x in red if it was\n",
    "        misclassified or in yellow if it was\n",
    "        classified correctly.\n",
    "    \"\"\"\n",
    "\n",
    "    xplot = np.linspace(-3, 3)\n",
    "    cs = np.array([x for x in 'gb'])\n",
    "\n",
    "    svm = - w[0] / w[1] * xplot - b / w[1]\n",
    "    left_svm = - w[0] / w[1] * xplot - b / w[1] - 1 / w[1]\n",
    "    right_svm = - w[0] / w[1] * xplot - b / w[1] + 1 / w[1]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],color=cs[labels].tolist(), s=50, alpha=0.8)\n",
    "    if error:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .2, color='r',fill=False))\n",
    "    else:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .2, color='y',fill=False))\n",
    "    ax.plot(xplot, left_svm, 'r--', lw=2)\n",
    "    ax.plot(xplot, svm, 'r-', lw=2)\n",
    "    ax.plot(xplot, right_svm, 'r--', lw=2)\n",
    "    ax.set_xlim(min(X[:, 0]) - 1, max(X[:,0]) + 1)\n",
    "    ax.set_ylim(min(X[:, 1]) - 1, max(X[:,1]) + 1)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def accuracy(w, b):\n",
    "    acc = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        ypred = np.dot(w, x) + b\n",
    "        if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "            acc += 1\n",
    "    return acc / len(X)\n",
    "\n",
    "images = []\n",
    "for _ in range(epochs):\n",
    "    # pick a point from X at random\n",
    "    i = np.random.randint(0, len(X))\n",
    "    x, y = X[i], Y[i]\n",
    "    error = False\n",
    "    ypred = np.dot(w, x) + b\n",
    "    if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "         # correctly classified\n",
    "        if (ypred <= 1 and ypred >= -1):\n",
    "            # but the point is in the street\n",
    "            w = w + alpha * y * x * retracting_rate\n",
    "            b += alpha * y * retracting_rate\n",
    "        else:\n",
    "            w = w * expanding_rate\n",
    "            b *= expanding_rate\n",
    "    else:\n",
    "        # misclassified\n",
    "        w = w + alpha * y * x * expanding_rate\n",
    "        b += alpha * y * expanding_rate\n",
    "    \n",
    "    acc = accuracy(w, b)\n",
    "    print('accuracy', acc)\n",
    "    images.append(snap(x, w, b, error))\n",
    "\n",
    "images[0].save(\n",
    "    'svm.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in class, the form\n",
    "$$w^T x + b = 0$$\n",
    "while simple, does not expose the inner product `<x_i, x_j>` which we know `w` depends on, having done the math. This is critical to applying the \"kernel trick\" which allows for learning non-linear decision boundaries. Let's modify the above algorithm to use the form\n",
    "$$\\sum_i \\alpha_i <x_i, x> + b = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "CENTERS = [[0, 1], [1, 0]]\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = .05\n",
    "expanding_rate = .99\n",
    "retracting_rate = 1.1\n",
    "\n",
    "X, labels = datasets.make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\n",
    "Y = np.array(list(map(lambda x : -1 if x == 0 else 1, labels.tolist())))\n",
    "\n",
    "alpha_i = np.zeros((len(X),))\n",
    "b = 0\n",
    "\n",
    "def snap(x, alpha_i, b, error):\n",
    "    # create a mesh to plot in\n",
    "    h = .01  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    meshData = np.c_[xx.ravel(), yy.ravel()]\n",
    "    cs = np.array([x for x in 'gb'])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],color=cs[labels].tolist(), s=50, alpha=0.8)\n",
    "\n",
    "    if error:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='r',fill=False))\n",
    "    else:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='y',fill=False))\n",
    "   \n",
    "    Z = predict_many(alpha_i, b, meshData)\n",
    "    Z = np.array([0 if z <=0 else 1 for z in Z]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=.5, cmap=plt.cm.Paired)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def predict_many(alpha_i, b, Z):\n",
    "    res = []\n",
    "    for i in range(len(Z)):\n",
    "        res.append(predict(alpha_i, b, Z[i]))\n",
    "    return np.array(res)\n",
    "\n",
    "def predict(alpha_i, b, x):\n",
    "    wx = 0\n",
    "    for i in range(len(X)):\n",
    "        wx += alpha_i[i] * np.dot(X[i], x)\n",
    "    return wx + b\n",
    "\n",
    "images = []\n",
    "for _ in range(epochs):\n",
    "    # pick a point from X at random\n",
    "    i = np.random.randint(0, len(X))\n",
    "    error = False\n",
    "    x, y = X[i], Y[i]\n",
    "    error = False\n",
    "    ypred = np.dot(w, x) + b\n",
    "    if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "         # correctly classified\n",
    "        if (ypred <= 1 and ypred >= -1):\n",
    "            # but the point is in the street\n",
    "            alpha_i[i] += learning_rate * y\n",
    "            b += learning_rate * y * retracting_rate\n",
    "            alpha_i = alpha_i * retracting_rate\n",
    "        else:\n",
    "            alpha_i = alpha_i * expanding_rate\n",
    "            b *= expanding_rate\n",
    "    else:\n",
    "        # misclassified\n",
    "        alpha_i[i] += learning_rate * y\n",
    "        b += learning_rate * y * expanding_rate\n",
    "        alpha_i = alpha_i * expanding_rate\n",
    "    \n",
    "    acc = accuracy(w, b)\n",
    "    print('accuracy', acc)\n",
    "        \n",
    "    images.append(snap(x, alpha_i, b, error))\n",
    "\n",
    "images[0].save(\n",
    "    'svm_dual.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a configurable kernel function to apply in lieu of the dot product. Try it out on a dataset that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x_i, x_j, c, n):\n",
    "    return (np.dot(x_i, x_j) + c) ** n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n",
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "CENTERS = [[0, 1], [1, 0]]\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = .05\n",
    "expanding_rate = .99\n",
    "retracting_rate = 1.1\n",
    "\n",
    "X, labels = datasets.make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\n",
    "Y = np.array(list(map(lambda x : -1 if x == 0 else 1, labels.tolist())))\n",
    "\n",
    "alpha_i = np.zeros((len(X),))\n",
    "b = 0\n",
    "\n",
    "def snap(x, alpha_i, b, error):\n",
    "    # create a mesh to plot in\n",
    "    h = .01  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    meshData = np.c_[xx.ravel(), yy.ravel()]\n",
    "    cs = np.array([x for x in 'gb'])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],color=cs[labels].tolist(), s=50, alpha=0.8)\n",
    "\n",
    "    if error:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='r',fill=False))\n",
    "    else:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='y',fill=False))\n",
    "   \n",
    "    Z = predict_many(alpha_i, b, meshData)\n",
    "    Z = np.array([0 if z <=0 else 1 for z in Z]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=.5, cmap=plt.cm.Paired)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def predict_many(alpha_i, b, Z):\n",
    "    res = []\n",
    "    for i in range(len(Z)):\n",
    "        res.append(predict(alpha_i, b, Z[i]))\n",
    "    return np.array(res)\n",
    "\n",
    "def predict(alpha_i, b, x):\n",
    "    wx = 0\n",
    "    for i in range(len(X)):\n",
    "        wx += alpha_i[i] * polynomial(X[i], x, 0, 2)\n",
    "    return wx + b\n",
    "\n",
    "images = []\n",
    "for _ in range(epochs):\n",
    "    # pick a point from X at random\n",
    "    i = np.random.randint(0, len(X))\n",
    "    error = False\n",
    "    x, y = X[i], Y[i]\n",
    "    error = False\n",
    "    ypred = np.dot(w, x) + b\n",
    "    if (ypred >= 0 and y > 0) or (ypred < 0 and y < 0):\n",
    "         # correctly classified\n",
    "        if (ypred <= 1 and ypred >= -1):\n",
    "            # but the point is in the street\n",
    "            alpha_i[i] += learning_rate * y\n",
    "            b += learning_rate * y * retracting_rate\n",
    "            alpha_i = alpha_i * retracting_rate\n",
    "        else:\n",
    "            alpha_i = alpha_i * expanding_rate\n",
    "            b *= expanding_rate\n",
    "    else:\n",
    "        # misclassified\n",
    "        alpha_i[i] += learning_rate * y\n",
    "        b += learning_rate * y * expanding_rate\n",
    "        alpha_i = alpha_i * expanding_rate\n",
    "    \n",
    "    acc = accuracy(w, b)\n",
    "    print('accuracy', acc)\n",
    "        \n",
    "    images.append(snap(x, alpha_i, b, error))\n",
    "\n",
    "images[0].save(\n",
    "    'svm_dual.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "In the example in class of recommending movies to users we used the movie rating as a measure of similarity between users and movies and thus the predicted rating for a user is a proxy for how highly a movie should be recommended. So the higher the predicted rating for a user, the higher a recommendation it would be.\n",
    "\n",
    "a) Consider a streaming platform that only has \"like\" or \"dislike\" (no 1-5 rating). Describe how you would build a recommender system in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will analyze the data separately,\n",
    "- Data exists for both users and movies  \n",
    "- Data only exists for movies  \n",
    "- Only have access to ratings  \n",
    "  \n",
    "And I will do the  feature extraction based on content(e.g. The ratings of different kind of movies from different users).  \n",
    "Then I will collaborate filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe 3 challenges of building a recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sclae (millions of users, millions of movies)  \n",
    "Cold start (change in user base, change in content)  \n",
    "Sparse data (not many users rank movies). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Why is SVD not an option for collaborative filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have to decompose R into P and Q, but R is sparse."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
